<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.1.0',
    sidebar: {"position":"left","display":"always","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="BERT Runtime最近继续怼BERT，项目大部分模型都上了BERT，真香啊。 本来一直在使用PyTorch JIT来解决加速和部署的问题，顺手还写了个service-streamer来做web和模型的中间件。正好上个月NVIDIA开源了基于TensorRT的BERT代码，官方blog号称单次inference只用2.2ms，比cpu快20倍。但是正确的问法是：这东西能比TF/PyTorch快">
<meta name="keywords" content="cuda,深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT Runtime">
<meta property="og:url" content="https://meteorix.github.io/2019/09/07/bert-runtime/index.html">
<meta property="og:site_name" content="Meteorix&#39;s Blog">
<meta property="og:description" content="BERT Runtime最近继续怼BERT，项目大部分模型都上了BERT，真香啊。 本来一直在使用PyTorch JIT来解决加速和部署的问题，顺手还写了个service-streamer来做web和模型的中间件。正好上个月NVIDIA开源了基于TensorRT的BERT代码，官方blog号称单次inference只用2.2ms，比cpu快20倍。但是正确的问法是：这东西能比TF/PyTorch快">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://meteorix.github.io/images/bert-runtime/gpucpu.png">
<meta property="og:image" content="https://meteorix.github.io/images/bert-runtime/bert.png">
<meta property="og:image" content="https://meteorix.github.io/images/bert-runtime/gelu.png">
<meta property="og:image" content="https://meteorix.github.io/images/bert-runtime/gelujit.png">
<meta property="og:image" content="https://meteorix.github.io/images/bert-runtime/std.jpg">
<meta property="og:image" content="https://meteorix.github.io/images/bert-runtime/qkv.png">
<meta property="og:image" content="https://meteorix.github.io/images/bert-runtime/async.png">
<meta property="og:updated_time" content="2019-09-07T11:52:42.586Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BERT Runtime">
<meta name="twitter:description" content="BERT Runtime最近继续怼BERT，项目大部分模型都上了BERT，真香啊。 本来一直在使用PyTorch JIT来解决加速和部署的问题，顺手还写了个service-streamer来做web和模型的中间件。正好上个月NVIDIA开源了基于TensorRT的BERT代码，官方blog号称单次inference只用2.2ms，比cpu快20倍。但是正确的问法是：这东西能比TF/PyTorch快">
<meta name="twitter:image" content="https://meteorix.github.io/images/bert-runtime/gpucpu.png">





  
  
  <link rel="canonical" href="https://meteorix.github.io/2019/09/07/bert-runtime/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>BERT Runtime | Meteorix's Blog</title>
  




  <script async src="//www.googletagmanager.com/gtag/js?id=UA-138766747-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-138766747-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Meteorix's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://meteorix.github.io/2019/09/07/bert-runtime/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="meteorix">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/qiyu.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Meteorix's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">BERT Runtime

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-09-07 15:03:33 / Modified: 19:52:42" itemprop="dateCreated datePublished" datetime="2019-09-07T15:03:33+08:00">2019-09-07</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="BERT-Runtime"><a href="#BERT-Runtime" class="headerlink" title="BERT Runtime"></a>BERT Runtime</h1><p>最近继续怼<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT</a>，项目大部分模型都上了BERT，真香啊。</p>
<p>本来一直在使用<code>PyTorch JIT</code>来解决加速和部署的问题，顺手还写了个<a href="https://github.com/ShannonAI/service-streamer" target="_blank" rel="noopener">service-streamer</a>来做web和模型的中间件。<br>正好上个月NVIDIA开源了基于<code>TensorRT</code>的<a href="https://github.com/NVIDIA/TensorRT/tree/release/5.1/demo/BERT" target="_blank" rel="noopener">BERT代码</a>，官方<a href="https://devblogs.nvidia.com/nlu-with-tensorrt-bert/" target="_blank" rel="noopener">blog</a>号称单次<code>inference</code>只用2.2ms，比cpu快20倍。但是正确的问法是：这东西能比TF/PyTorch快多少呢？</p>
<p>于是从<a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="noopener">TensorRT</a>开始，认真学习了一波NVIDIA的BERT实现。并做了性能Benchmark对比TensorFlow和PyTorch，结论是gpu时间能快<strong>15%-30%</strong>。主要归因于对BERT的计算图优化，自己实现了4个cuda kernel，另外避免了TensorFlow和PyTorch等框架带来的overhead。</p>
<h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><p>比较有用的几个背景知识：</p>
<ol>
<li>当然是BERT的<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">Paper</a>，<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">Tensorflow实现</a>，<a href="https://github.com/huggingface/pytorch-transformers" target="_blank" rel="noopener">PyTorch实现</a></li>
<li>Harvard写的著名解读<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></li>
<li>GPU和Cuda基础知识，很简单可以参考我的<a href="https://github.com/Meteorix/meteorix-blog/blob/master/_posts/cuda101.md" target="_blank" rel="noopener">cuda101</a></li>
</ol>
<h2 id="TensorRT"><a href="#TensorRT" class="headerlink" title="TensorRT"></a>TensorRT</h2><p><strong>TensorRT</strong>是NVIDIA官方推出的inference引擎，建立在CUDA之上。可以对<code>TensorFlow/PyTorch</code>等框架训练出来的模型进行CUDA优化，达到更高的inference性能。同时支持低精度参数、跨平台部署等，总之就是对自己家的GPU使用的最好。</p>
<a id="more"></a>
<p>跟<a href="https://github.com/NVIDIA/TensorRT" target="_blank" rel="noopener">TensorRT</a>的编译斗争了一两天，整体还是比较顺畅，照着<code>README</code>：</p>
<ol>
<li>准备环境，常规c++/py编译环境和cuda环境，我是<code>Titan XP + cuda-10.0 + cuDNN-7.4</code></li>
<li>下载TensorRT的binary release。TensorRT本身并没有开源，而是提供了编译好的lib。开源的周边代码包括：<ul>
<li><code>include</code>头文件</li>
<li><code>plugin</code>实现一些cuda扩展</li>
<li><code>parser</code>实现不同格式模型文件的解析</li>
</ul>
</li>
<li>Docker build编译用的镜像。</li>
<li>在Docker容器里面编译TensorRT的lib和开源代码。</li>
</ol>
<h2 id="TensorRT-BERT"><a href="#TensorRT-BERT" class="headerlink" title="TensorRT BERT"></a>TensorRT BERT</h2><p>TensorRT的BERT实现代码在<a href="https://github.com/NVIDIA/TensorRT/tree/release/5.1/demo/BERT" target="_blank" rel="noopener">demo/BERT</a>目录下，主要提供了：</p>
<ol>
<li>针对BERT进行了4个计算图优化，用cuda实现了几个fusion的kernel，封装成TensorRT的plugin</li>
<li>TensorFlow模型文件转TensorRT模型文件的脚本</li>
<li>C++和python版API和完整的BERT inference代码。</li>
</ol>
<p>还是看<code>README</code>，以<code>SQuAD(QA)</code>模型为例提供了完整的使用步骤：</p>
<ol>
<li>下载BERT在SQuAD上finetune的TF模型文件，或者你也可以用自己finetune的模型文件</li>
<li>使用转换脚本将TF模型文件转换成TensorRT模型文件</li>
<li>使用另一个脚本将模型、参数、输入问题转换为Tensor形式的输入输出</li>
<li>编译C++可执行文件，即可测试加速后的模型和输入输出，并保存为<code>bert.engine</code></li>
</ol>
<p>这个<code>bert.engine</code>文件，就可以单独使用了。既可以用C++ API或Python API加载后使用，也可以使用TensorRT Serving的docker直接加载做service。</p>
<h3 id="Python-API"><a href="#Python-API" class="headerlink" title="Python API"></a>Python API</h3><p>NVIDIA也提供了Python API来完成上面的几个步骤，需要多编译一些python binding。不过既然我都编好了C++版本，就只用Python API做inference。后面测试结果可以看出，Python API在模型inference的性能上与C++版本比几乎没有损耗。</p>
<p>Python API的使用依赖<a href="https://developer.nvidia.com/pycuda" target="_blank" rel="noopener">pycuda</a>，这是另一个官方库，用来做Python与CUDA之间的直接交互。这里包括分配显存、内存与显存之间copy tensor等。读取<code>bert.engine</code>执行inference则是使用TensorRT发布的whl包。</p>
<h3 id="复现NVIDIA提供的性能数据"><a href="#复现NVIDIA提供的性能数据" class="headerlink" title="复现NVIDIA提供的性能数据"></a>复现NVIDIA提供的性能数据</h3><p>NVIDIA官方数据是在<code>batchsize=1，seqlen=128</code>时测试的。在我们的Titan XP上分别使用C++和Python API，GPU时间都在<code>2.6ms</code>左右，基本复现了官方数据。</p>
<p><img src="/images/bert-runtime/gpucpu.png" alt="gpucpu.png"></p>
<p>比较有意思的是，明明与pytorch和tensorflow等框架比更能说明bert优化的效果，可能是为了diss cpu好卖gpu卡吧 :P</p>
<p>下面我们就来正经做一下Benchmark</p>
<h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><p>对于BERT的inference，很大一部分时间消耗在预处理上，即将输入的文字<code>tokenize</code>为<code>index</code>，执行<code>padding</code>和<code>masking</code>，再组装成<code>tensor</code>。而我们这里的benchmark只关心GPU执行inference的性能。所以我们的计时代码只包含GPU时间，也就是tensor输入到输出的时间，排除掉前后处理时间，另外包含tensor在CPU和GPU之间copy的时间。</p>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p><strong>GPU版本</strong></p>
<ul>
<li>GPU Titan XP</li>
<li>Cuda 10.0</li>
<li>Cudnn 7.5</li>
</ul>
<p><strong>Python3.6版本</strong></p>
<ul>
<li>Torch==1.2.0</li>
<li>TensorFlow==1.14.0</li>
<li>tensorrt==5.1.5.0</li>
</ul>
<p><strong>BERT实现</strong></p>
<ul>
<li>tensorrt基于 <a href="https://github.com/NVIDIA/TensorRT/tree/release/5.1/demo/BERT" target="_blank" rel="noopener">https://github.com/NVIDIA/TensorRT/tree/release/5.1/demo/BERT</a></li>
<li>TensorFlow基于 <a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a></li>
<li>PyTorch基于 <a href="https://github.com/huggingface/pytorch-transformers" target="_blank" rel="noopener">https://github.com/huggingface/pytorch-transformers</a></li>
</ul>
<p><strong>模型</strong></p>
<ul>
<li>bert-base 12层，SQuQA finetuned</li>
<li>相同的模型参数，分别转换为tensorrt/tf/pytorch模型文件</li>
</ul>
<h3 id="SQuAD任务"><a href="#SQuAD任务" class="headerlink" title="SQuAD任务"></a>SQuAD任务</h3><p>使用SQuAD(QA)任务进行测试<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 输入文章和问题</span><br><span class="line">Passage: TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.</span><br><span class="line"></span><br><span class="line">Question: What is TensorRT?</span><br><span class="line"></span><br><span class="line"># 输出答案</span><br><span class="line">Answer: &apos;a high performance deep learning inference platform&apos;</span><br></pre></td></tr></table></figure></p>
<p>使用上面的QA任务样例，输入<code>padding</code>到<code>Sequence Length=328</code>，<code>Batch Size</code>分别使用<code>1</code>和<code>32</code>。测量100次取平均单句时间，单位是<code>ms</code></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><table>
<thead>
<tr>
<th>bs * seqlen</th>
<th>tensorrt c++</th>
<th>tensorrt py</th>
<th>tensorflow</th>
<th>pytorch</th>
<th>pytorch jit</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 * 328</td>
<td>9.9</td>
<td>9.9</td>
<td>17</td>
<td>16.3</td>
<td>14.8</td>
</tr>
<tr>
<td>32 * 328</td>
<td>7.3</td>
<td></td>
<td>11.6</td>
<td>9.9</td>
<td>8.6</td>
</tr>
</tbody>
</table>
<p>注：</p>
<ol>
<li>TensorFlow接口封装不太熟悉，仅供参考，目测与PyTorch无jit版本性能差不多</li>
<li>TensorRT py接口暂时没实现多batch的inference，目测与c++版本性能差不多</li>
<li>所有测试GPU利用率都接近<code>100%</code>，说明没有什么GPU之外的阻塞代码</li>
</ol>
<p>结论：</p>
<ol>
<li>TensorRT比PyTorch快39%-26% </li>
<li>TensorRT比PyTorch jit快33%-16%</li>
</ol>
<h2 id="计算图优化和kernel优化"><a href="#计算图优化和kernel优化" class="headerlink" title="计算图优化和kernel优化"></a>计算图优化和kernel优化</h2><p>那么我们来看看TensorRT实现的BERT，到底做了哪些优化。</p>
<p><img src="/images/bert-runtime/bert.png" alt="bert.png"></p>
<p>上面的计算图给了一个BERT <code>Transformer Encoder</code>的总览。对<code>Transformer</code>还不熟悉的话，可以回头看看Harvard写的著名解读<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a>。总共有4点计算图优化，3点在<code>Transformer</code>中：</p>
<ol>
<li><code>gelu</code>激活函数的kernel实现</li>
<li><code>skip</code>和<code>layernorm</code>函数的fusion</li>
<li><code>Q/K/V</code>三个矩阵的合并乘法和转置</li>
</ol>
<p>上面的前3个优化在12层<code>Transformer</code>中都会用到，所以性价比很高。第4点优化在最底层<code>BERT Embedding</code>层：</p>
<ol start="4">
<li><code>embedding</code>和<code>layernorm</code>的fusion</li>
</ol>
<p>下面分别看看4处优化是如何实现的，我也是趁此机会了解计算图优化和cuda kernel函数的编写。</p>
<h3 id="Gelu"><a href="#Gelu" class="headerlink" title="Gelu"></a>Gelu</h3><p>按照<code>gelu</code>的公式，如果每步分开计算，每步kernel调用都会进行一次global显存的读写。</p>
<p><img src="/images/bert-runtime/gelu.png" alt="gelu.png"></p>
<blockquote>
<p>由于gpu的硬件特性，<code>global memory</code>的访问速度非常慢（相对计算而言），这里可以参考前一篇笔记中的<a href="https://github.com/Meteorix/meteorix-blog/blob/master/_posts/cuda101.md#gpu%E8%AE%BE%E8%AE%A1" target="_blank" rel="noopener">gpu设计和内存结构</a>。</p>
</blockquote>
<p>于是TensorRT就写一个gelu的kernel，一次kernel函数调用解决问题，只用一次显存读写。</p>
<p><a href="https://github.com/NVIDIA/TensorRT/blob/release/5.1/demo/BERT/plugins/geluPlugin.cu" target="_blank" rel="noopener">https://github.com/NVIDIA/TensorRT/blob/release/5.1/demo/BERT/plugins/geluPlugin.cu</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// constants for approximating the normal cdf</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">float</span> A = <span class="number">0.5</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">float</span> B = <span class="number">0.7978845608028654</span>; <span class="comment">// sqrt(2.0/M_PI)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">constexpr</span> <span class="keyword">float</span> C = <span class="number">0.035677408136300125</span>; <span class="comment">// 0.044715 * sqrt(2.0/M_PI)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">unsigned</span> TPB&gt;</span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">geluKernel</span><span class="params">(<span class="keyword">const</span> T a, <span class="keyword">const</span> T b, <span class="keyword">const</span> T c, <span class="keyword">int</span> n, <span class="keyword">const</span> T* input, T* output)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> idx = blockIdx.x * TPB + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (idx &lt; n)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">const</span> T in = input[idx];</span><br><span class="line">        <span class="keyword">const</span> T cdf = a + a * <span class="built_in">tanh</span>(in * (c * in * in + b));</span><br><span class="line">        output[idx] = in * cdf;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对比PyTorch JIT</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@torch.jit.script</span><br><span class="line">def gelu(x):</span><br><span class="line">    return 0.5 * x * (1 + torch.tanh(0.797884 * (x + 0.044715 * torch.pow(x, 3))))</span><br><span class="line"></span><br><span class="line">print(gelu.graph)</span><br></pre></td></tr></table></figure>
<p><img src="/images/bert-runtime/gelujit.png" alt="gelujit.png"></p>
<p>从计算图上看确实每一步是单独计算，除了<code>tanh</code>这种内置的函数，其他都要一层层函数调用。</p>
<p>不过，在PyTorch 1.2的最新代码中，我发现<code>gelu</code>也是用了内置的cuda实现，两者几乎等价。</p>
<h3 id="Skip-and-Layer-Normalization"><a href="#Skip-and-Layer-Normalization" class="headerlink" title="Skip and Layer-Normalization"></a>Skip and Layer-Normalization</h3><p>LayerNorm层的PyTorch实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class LayerNorm(nn.Module):</span><br><span class="line">    &quot;Construct a layernorm module (See citation for details).&quot;</span><br><span class="line">    def __init__(self, features, eps=1e-6):</span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        mean = x.mean(-1, keepdim=True)</span><br><span class="line">        std = x.std(-1, keepdim=True)</span><br><span class="line">        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure></p>
<p>忽略掉几个不重要的参数，主要是计算<code>mean</code>和<code>std</code>，各需要遍历一次所有输入参数。</p>
<p>加上<code>LayerNorm</code>之前的<code>Skip</code>层，一共需要遍历三次所有输入参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = LayerNorm(x + Sublayer(x))</span><br></pre></td></tr></table></figure>
<p>根据上面说的GPU硬件和显存特性，启动三次kernel函数、遍历三次，都是消耗较大的。<br>所以优化为：</p>
<ol>
<li>算<code>Skip</code>层的同时计算<code>x</code>和<code>x^2</code>的平均值</li>
<li><p>再算<code>LayerNorm</code>层时直接用<code>x</code>和<code>x^2</code>的平均值得到<code>mean</code>和<code>std</code></p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std = sqrt(mean(x^<span class="number">2</span>) - mean(x)^<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>看代码的时候没明白，跟yuxian手推了一波这个公式（逃</p>
<p><img src="/images/bert-runtime/std.jpg" alt="std.jpg"></p>
<p>这样将三次遍历fusion成一次，省去了读写global显存的时间</p>
<p>cuda代码实现：</p>
<p><a href="https://github.com/NVIDIA/TensorRT/blob/release/5.1/demo/BERT/plugins/skipLayerNormPlugin.cu" target="_blank" rel="noopener">https://github.com/NVIDIA/TensorRT/blob/release/5.1/demo/BERT/plugins/skipLayerNormPlugin.cu</a></p>
<p>类似的还有<code>Embeding+LN</code>的fusion，理论上所有<code>LN</code>前面有一次遍历的都可以先算出来<code>x</code>和<code>x^2</code>的均值，省去两次遍历：</p>
<p><a href="https://github.com/NVIDIA/TensorRT/blob/release/5.1/demo/BERT/plugins/embLayerNormPlugin.cu" target="_blank" rel="noopener">https://github.com/NVIDIA/TensorRT/blob/release/5.1/demo/BERT/plugins/embLayerNormPlugin.cu</a></p>
<h3 id="QKV-优化"><a href="#QKV-优化" class="headerlink" title="QKV 优化"></a>QKV 优化</h3><p>有了上面的基础，这里的两个优化比较容易理解，直接看图和代码</p>
<p><img src="/images/bert-runtime/qkv.png" alt="qkv.png"></p>
<p>1）<code>QKV</code>本来是分别成三个矩阵然后转置，现在变成成一个三倍大的矩阵转置，再slice</p>
<p><a href="https://github.com/NVIDIA/TensorRT/blob/release/5.1/demo/BERT/plugins/qkvToContextPlugin.cu" target="_blank" rel="noopener">https://github.com/NVIDIA/TensorRT/blob/release/5.1/demo/BERT/plugins/qkvToContextPlugin.cu</a></p>
<p>2）<code>Scale+Softmax</code>，在scale那一次遍历同时求得<code>exp(x)</code>，减少一次遍历</p>
<p><a href="https://github.com/NVIDIA/TensorRT/blob/e47febadb256d94f65efe0f1eac54c7caedd65d4/demo/BERT/plugins/pluginUtil.h#L220" target="_blank" rel="noopener">https://github.com/NVIDIA/TensorRT/blob/e47febadb256d94f65efe0f1eac54c7caedd65d4/demo/BERT/plugins/pluginUtil.h#L220</a></p>
<h3 id="异步执行"><a href="#异步执行" class="headerlink" title="异步执行"></a>异步执行</h3><p>TensorRT的blog特别提了一下异步执行。由于CPU和GPU是异构的，在CPU和GPU之间copy tensor、GPU runtime执行计算都是异步完成的。不强制同步可以增加整个流程的吞吐量<code>througput</code>。Profile的时候需要特别注意这个异步的时间。这点在TensorRT的python代码中也能看到，实现的非常仔细。</p>
<p><img src="/images/bert-runtime/async.png" alt="async.png"></p>
<p>PyTorch实际上也是异步的，所以这点TensorRT没什么优势</p>
<h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h2><p>分析完TensorRT的BERT优化，我们看看能怎么用起来。</p>
<p>这30%左右的inference速度提升还是很香的，可能的用法有：</p>
<ol>
<li>使用Python API，替换tf/pytorch的BERT实现，前后处理代码不用动</li>
<li>使用C++ API，封装前后处理C++代码，编译成二进制发布</li>
<li>直接使用<a href="https://github.com/NVIDIA/tensorrt-inference-server" target="_blank" rel="noopener">tensorrt-inference-server</a>，server只处理tensor，前后处理需要另外实现</li>
</ol>
<p>这三种用法都需要将tf/pytorch训练(finetune)好的模型文件，转化为tensorrt的<code>.engine</code>文件：</p>
<ol>
<li>转换模型参数，每个任务的模型BERT最上层会稍有不同</li>
<li>确定输入输出、batch_size等参数，生成tensor文件</li>
<li>用前两部的结果生成<code>.engine</code>文件</li>
</ol>
<h3 id="So-what’s-next"><a href="#So-what’s-next" class="headerlink" title="So, what’s next?"></a>So, what’s next?</h3><p>根据项目的发展的阶段，考虑采用三种用法，主要先理顺<code>模型迭代--业务开发--部署</code>的流程。</p>
<blockquote>
<p>再次感叹BERT真香，NLP领域幸好有BERT，才能搞这些优化。</p>
</blockquote>

      
    </div>

    
      


    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/cuda/" rel="tag"># cuda</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/02/cuda101/" rel="next" title="cuda 101">
                <i class="fa fa-chevron-left"></i> cuda 101
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/qiyu.jpg" alt="meteorix">
            
              <p class="site-author-name" itemprop="name">meteorix</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">11</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">8</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/meteorix" title="GitHub &rarr; https://github.com/meteorix" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#BERT-Runtime"><span class="nav-number">1.</span> <span class="nav-text">BERT Runtime</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Prerequisite"><span class="nav-number">1.1.</span> <span class="nav-text">Prerequisite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorRT"><span class="nav-number">1.2.</span> <span class="nav-text">TensorRT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorRT-BERT"><span class="nav-number">1.3.</span> <span class="nav-text">TensorRT BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Python-API"><span class="nav-number">1.3.1.</span> <span class="nav-text">Python API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#复现NVIDIA提供的性能数据"><span class="nav-number">1.3.2.</span> <span class="nav-text">复现NVIDIA提供的性能数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Benchmark"><span class="nav-number">1.4.</span> <span class="nav-text">Benchmark</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#环境"><span class="nav-number">1.4.1.</span> <span class="nav-text">环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQuAD任务"><span class="nav-number">1.4.2.</span> <span class="nav-text">SQuAD任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#结论"><span class="nav-number">1.4.3.</span> <span class="nav-text">结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算图优化和kernel优化"><span class="nav-number">1.5.</span> <span class="nav-text">计算图优化和kernel优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Gelu"><span class="nav-number">1.5.1.</span> <span class="nav-text">Gelu</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Skip-and-Layer-Normalization"><span class="nav-number">1.5.2.</span> <span class="nav-text">Skip and Layer-Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#QKV-优化"><span class="nav-number">1.5.3.</span> <span class="nav-text">QKV 优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#异步执行"><span class="nav-number">1.5.4.</span> <span class="nav-text">异步执行</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何使用"><span class="nav-number">1.6.</span> <span class="nav-text">如何使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#So-what’s-next"><span class="nav-number">1.6.1.</span> <span class="nav-text">So, what’s next?</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">meteorix</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.1.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/affix.js?v=7.1.0"></script>

  <script src="/js/schemes/pisces.js?v=7.1.0"></script>



  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  


  


  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
